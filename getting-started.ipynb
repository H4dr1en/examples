{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "colab": {
      "name": "getting-started.ipynb",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HgljXEAJEcFq"
      },
      "source": [
        "# Getting Started"
      ],
      "id": "HgljXEAJEcFq"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P9VjKOirEcFu"
      },
      "source": [
        "Welcome to **PyTorch-Ignite**’s quick start guide that covers the\n",
        "essentials of getting a project up and running while walking through\n",
        "basic concepts of Ignite. In just a few lines of code, you can get your\n",
        "model trained and validated. The complete code can be found at the end\n",
        "of this guide."
      ],
      "id": "P9VjKOirEcFu"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1QNvbg3SEcFw"
      },
      "source": [
        "## Prerequisites\n",
        "\n",
        "This tutorial assumes you are familiar with the:\n",
        "\n",
        "1.  Basics of Python and deep learning\n",
        "2.  Structure of PyTorch code"
      ],
      "id": "1QNvbg3SEcFw"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XTHzzYyoEcFy"
      },
      "source": [
        "## Installation\n",
        "\n",
        "1.  From `pip`\n",
        "\n",
        "``` shell\n",
        "pip install pytorch-ignite\n",
        "```\n",
        "\n",
        "1.  From `conda`\n",
        "\n",
        "``` shell\n",
        "conda install ignite -c pytorch\n",
        "```\n",
        "\n",
        "See [here](https://pytorch-ignite.netlify.app/docs/how-to-guides/installation) for other installation\n",
        "options."
      ],
      "id": "XTHzzYyoEcFy"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DcnSr5sGEcFz"
      },
      "source": [
        "## Code\n",
        "\n",
        "Import the following:"
      ],
      "id": "DcnSr5sGEcFz"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Saizk3heEcFz"
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.optim import SGD\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.datasets import MNIST\n",
        "from torchvision.models import resnet18\n",
        "from torchvision.transforms import Compose, Normalize, ToTensor\n",
        "from tqdm import tqdm\n",
        "\n",
        "from ignite.engine import Engine, Events, create_supervised_trainer, create_supervised_evaluator\n",
        "from ignite.metrics import Accuracy, Loss"
      ],
      "id": "Saizk3heEcFz",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ecMYtJF7OvgT"
      },
      "source": [
        "Speed things up by setting [device](https://pytorch.org/docs/stable/tensor_attributes.html#torch.torch.device) to `cuda` if available else `cpu`."
      ],
      "id": "ecMYtJF7OvgT"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sdjDKcFhOuQn"
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "id": "sdjDKcFhOuQn",
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4r_PUH1yEcF1"
      },
      "source": [
        "Define a class of your model or use the predefined ResNet18 model (modified for MNIST) below, instantiate it and move it to device:"
      ],
      "id": "4r_PUH1yEcF1"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dVSVAT0OEcF1"
      },
      "source": [
        "class Net(nn.Module):\n",
        "\n",
        "    def __init__(self, in_channels=1):\n",
        "        super(Net, self).__init__()\n",
        "        \n",
        "        # Changed the output layer to output 10 classes instead of 1000 classes\n",
        "        self.model = resnet18(num_classes=10)\n",
        "\n",
        "        # Changed the input layer to take grayscale images for MNIST instaed of RGB images\n",
        "        self.model.conv1 = nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "model = Net().to(device)"
      ],
      "id": "dVSVAT0OEcF1",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DDIW2zedEcF3"
      },
      "source": [
        "Now let us define the training and validation datasets (as\n",
        "[torch.utils.data.DataLoader](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader))\n",
        "and store them in `train_loader` and `val_loader` respectively. We have\n",
        "used the [MNIST](https://pytorch.org/vision/stable/datasets.html#mnist)\n",
        "dataset for ease of understanding.\n"
      ],
      "id": "DDIW2zedEcF3"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PFNgx_-TEcF4"
      },
      "source": [
        "def get_data_loaders(train_batch_size, val_batch_size):\n",
        "    # normalizing the data\n",
        "    data_transform = Compose([ToTensor(), Normalize((0.1307,), (0.3081,))])\n",
        "\n",
        "    train_loader = DataLoader(\n",
        "        MNIST(download=True, root=\".\", transform=data_transform, train=True), batch_size=train_batch_size, shuffle=True\n",
        "    )\n",
        "\n",
        "    val_loader = DataLoader(\n",
        "        MNIST(download=True, root=\".\", transform=data_transform, train=False), batch_size=val_batch_size, shuffle=False\n",
        "    )\n",
        "    return train_loader, val_loader\n",
        "\n",
        "train_loader, val_loader = get_data_loaders(train_batch_size=128, val_batch_size=256)"
      ],
      "id": "PFNgx_-TEcF4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VC9BUtWXEcF6"
      },
      "source": [
        "Finally, we will specify the optimizer and the loss function:"
      ],
      "id": "VC9BUtWXEcF6"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6VkGmtVZEcF7"
      },
      "source": [
        "optimizer = torch.optim.RMSprop(model.parameters(), lr=0.005)\n",
        "criterion = nn.CrossEntropyLoss()"
      ],
      "id": "6VkGmtVZEcF7",
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cb-ak9gEEcF7"
      },
      "source": [
        "And we’re done with setting up the important parts of the project.\n",
        "PyTorch-Ignite will handle all other boilerplate code as we will see\n",
        "below. Next we have to define a trainer engine by passing our model,\n",
        "optimizer and loss function to\n",
        "[`create_supervised_trainer`](https://pytorch.org/ignite/generated/ignite.engine.create_supervised_trainer.html),\n",
        "and an evaluator engine by passing Ignite’s out-of-the-box\n",
        "[metrics](https://pytorch.org/ignite/metrics.html#complete-list-of-metrics)\n",
        "and the model to\n",
        "[`create_supervised_evaluator`](https://pytorch.org/ignite/v0.4.5/generated/ignite.engine.create_supervised_evaluator.html#create-supervised-evaluator)\n",
        ":"
      ],
      "id": "cb-ak9gEEcF7"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NufcPqJaEcF8"
      },
      "source": [
        "trainer = create_supervised_trainer(model, optimizer, criterion, device)\n",
        "\n",
        "val_metrics = {\n",
        "    \"accuracy\": Accuracy(),\n",
        "    \"nll\": Loss(criterion)\n",
        "}\n",
        "evaluator = create_supervised_evaluator(model, metrics=val_metrics, device=device)"
      ],
      "id": "NufcPqJaEcF8",
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S7YThetiEcF8"
      },
      "source": [
        "Both `trainer` and `evaluator` objects are instances of\n",
        "[`Engine`](https://pytorch.org/ignite/v0.4.5/generated/ignite.engine.engine.Engine.html#ignite.engine.engine.Engine) - the main component of Ignite, which is essentially an abstraction over\n",
        "the training or validation loop.\n",
        "\n",
        "If you need more control over your training and validation loops, you\n",
        "can create custom `trainer` and `evaluator` objects by wrapping the step\n",
        "logic in `Engine` :\n",
        "\n",
        "```python\n",
        "def train_step(engine, batch):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    x, y = batch[0].to(device), batch[1].to(device)\n",
        "    y_pred = model(x)\n",
        "    loss = criterion(y_pred, y)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    return loss.item()\n",
        "\n",
        "trainer = Engine(train_step)\n",
        "\n",
        "def validation_step(engine, batch):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        x, y = batch[0].to(device), batch[1].to(device)\n",
        "        y_pred = model(x)\n",
        "        return y_pred, y\n",
        "\n",
        "evaluator = Engine(validation_step)\n",
        "\n",
        "# Attach metrics to the evaluator\n",
        "for name, metric in val_metrics.items():\n",
        "    metric.attach(evaluator, name)\n",
        "```"
      ],
      "id": "S7YThetiEcF8"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sw90sOK9EcF9"
      },
      "source": [
        "We can customize the code further by adding all kinds of event handlers.\n",
        "`Engine` allows adding handlers on various events that are triggered\n",
        "during the run. When an event is triggered, attached handlers\n",
        "(functions) are executed. Thus, for logging purposes we add a function\n",
        "to be executed at the end of every `log_interval`-th iteration:"
      ],
      "id": "Sw90sOK9EcF9"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YGm_-loUEcF9"
      },
      "source": [
        "# How many batches to wait before logging training status\n",
        "log_interval=100"
      ],
      "id": "YGm_-loUEcF9",
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V3xpFBI6EcF9"
      },
      "source": [
        "@trainer.on(Events.ITERATION_COMPLETED(every=log_interval))\n",
        "def log_training_loss(engine):\n",
        "    print(f\"Epoch[{engine.state.epoch}] Loss: {engine.state.output:.2f}\")"
      ],
      "id": "V3xpFBI6EcF9",
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O6uwwXO8EcF-"
      },
      "source": [
        "or equivalently without the decorator but attaching the handler function\n",
        "to the `trainer` via\n",
        "[`add_event_handler`](https://pytorch.org/ignite/v0.4.5/generated/ignite.engine.engine.Engine.html#ignite.engine.engine.Engine.add_event_handler)\n",
        "\n",
        "``` python\n",
        "def log_training_loss(engine):\n",
        "    print(f\"Epoch[{engine.state.epoch}] Loss: {engine.state.output:.2f}\")\n",
        "\n",
        "trainer.add_event_handler(Events.ITERATION_COMPLETED, log_training_loss)\n",
        "```"
      ],
      "id": "O6uwwXO8EcF-"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "quQzbAv6EcF-"
      },
      "source": [
        "After an epoch ends during training, we can compute the training and\n",
        "validation metrics by running `evaluator` on `train_loader` and\n",
        "`val_loader`. Hence we will attach two additional handlers to `trainer`\n",
        "when an epoch completes:"
      ],
      "id": "quQzbAv6EcF-"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eCE552PFEcF_"
      },
      "source": [
        "@trainer.on(Events.EPOCH_COMPLETED)\n",
        "def log_training_results(trainer):\n",
        "    evaluator.run(train_loader)\n",
        "    metrics = evaluator.state.metrics\n",
        "    print(f\"Training Results - Epoch[{trainer.state.epoch}] Avg accuracy: {metrics['accuracy']:.2f} Avg loss: {metrics['nll']:.2f}\")\n",
        "\n",
        "@trainer.on(Events.EPOCH_COMPLETED)\n",
        "def log_validation_results(trainer):\n",
        "    evaluator.run(val_loader)\n",
        "    metrics = evaluator.state.metrics\n",
        "    print(f\"Validation Results - Epoch[{trainer.state.epoch}] Avg accuracy: {metrics['accuracy']:.2f} Avg loss: {metrics['nll']:.2f}\")"
      ],
      "id": "eCE552PFEcF_",
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aq0qwiZrEcF_"
      },
      "source": [
        "Finally, we start the engine on the training dataset and run it for 5\n",
        "epochs:"
      ],
      "id": "Aq0qwiZrEcF_"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qnmTh4FeEcGA",
        "outputId": "10a7e006-566d-4d3c-cc44-8d1d7464e47c"
      },
      "source": [
        "trainer.run(train_loader, max_epochs=5)"
      ],
      "id": "qnmTh4FeEcGA",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
            "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch[1] Loss: 0.62\n",
            "Epoch[1] Loss: 0.16\n",
            "Epoch[1] Loss: 0.05\n",
            "Epoch[1] Loss: 0.32\n",
            "Training Results - Epoch[1] Avg accuracy: 0.97 Avg loss: 0.11\n",
            "Validation Results - Epoch[1] Avg accuracy: 0.96 Avg loss: 0.11\n",
            "Epoch[2] Loss: 0.12\n",
            "Epoch[2] Loss: 0.07\n",
            "Epoch[2] Loss: 0.10\n",
            "Epoch[2] Loss: 0.06\n",
            "Epoch[2] Loss: 0.01\n",
            "Training Results - Epoch[2] Avg accuracy: 0.96 Avg loss: 0.13\n",
            "Validation Results - Epoch[2] Avg accuracy: 0.96 Avg loss: 0.15\n",
            "Epoch[3] Loss: 0.01\n",
            "Epoch[3] Loss: 0.03\n",
            "Epoch[3] Loss: 0.08\n",
            "Epoch[3] Loss: 0.07\n",
            "Epoch[3] Loss: 0.06\n",
            "Training Results - Epoch[3] Avg accuracy: 0.98 Avg loss: 0.06\n",
            "Validation Results - Epoch[3] Avg accuracy: 0.98 Avg loss: 0.06\n",
            "Epoch[4] Loss: 0.05\n",
            "Epoch[4] Loss: 0.03\n",
            "Epoch[4] Loss: 0.01\n",
            "Epoch[4] Loss: 0.05\n",
            "Training Results - Epoch[4] Avg accuracy: 0.98 Avg loss: 0.07\n",
            "Validation Results - Epoch[4] Avg accuracy: 0.98 Avg loss: 0.07\n",
            "Epoch[5] Loss: 0.65\n",
            "Epoch[5] Loss: 0.02\n",
            "Epoch[5] Loss: 0.01\n",
            "Epoch[5] Loss: 0.06\n",
            "Epoch[5] Loss: 0.01\n",
            "Training Results - Epoch[5] Avg accuracy: 0.99 Avg loss: 0.03\n",
            "Validation Results - Epoch[5] Avg accuracy: 0.99 Avg loss: 0.04\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "State:\n",
              "\titeration: 2345\n",
              "\tepoch: 5\n",
              "\tepoch_length: 469\n",
              "\tmax_epochs: 5\n",
              "\toutput: 0.053093891590833664\n",
              "\tbatch: <class 'list'>\n",
              "\tmetrics: <class 'dict'>\n",
              "\tdataloader: <class 'torch.utils.data.dataloader.DataLoader'>\n",
              "\tseed: <class 'NoneType'>\n",
              "\ttimes: <class 'dict'>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wJ9k2coEEcGD"
      },
      "source": [
        "## Next Steps\n",
        "\n",
        "1.  Check out [tutorials](/docs/tutorials) if you want to continue\n",
        "    learning more about PyTorch-Ignite.\n",
        "2.  Head over to [how-to guides](/docs/how-to-guides) if you’re looking\n",
        "    for a specific problem.\n",
        "3.  If you want to set-up a PyTorch-Ignite project, visit [Code\n",
        "    Generator](https://code-generator.netlify.app/) to get a variety of\n",
        "    easily customizable templates and out-of-the-box features."
      ],
      "id": "wJ9k2coEEcGD"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "vya75pqVEcGE"
      },
      "source": [
        "## Complete Code\n",
        "\n",
        "``` python\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.optim import SGD\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.datasets import MNIST\n",
        "from torchvision.models import resnet18\n",
        "from torchvision.transforms import Compose, Normalize, ToTensor\n",
        "from tqdm import tqdm\n",
        "\n",
        "from ignite.engine import Engine, Events, create_supervised_trainer, create_supervised_evaluator\n",
        "from ignite.metrics import Accuracy, Loss\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "class Net(nn.Module):\n",
        "\n",
        "    def __init__(self, in_channels=1):\n",
        "        super(Net, self).__init__()\n",
        "        \n",
        "        # Changed the output layer to output 10 classes instead of 1000 classes\n",
        "        self.model = resnet18(num_classes=10)\n",
        "\n",
        "        # Changed the input layer to take grayscale images for MNIST instaed of RGB images\n",
        "        self.model.conv1 = nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "model = Net().to(device)\n",
        "\n",
        "def get_data_loaders(train_batch_size, val_batch_size):\n",
        "    # normalizing the data\n",
        "    data_transform = Compose([ToTensor(), Normalize((0.1307,), (0.3081,))])\n",
        "\n",
        "    train_loader = DataLoader(\n",
        "        MNIST(download=True, root=\".\", transform=data_transform, train=True), batch_size=train_batch_size, shuffle=True\n",
        "    )\n",
        "\n",
        "    val_loader = DataLoader(\n",
        "        MNIST(download=True, root=\".\", transform=data_transform, train=False), batch_size=val_batch_size, shuffle=False\n",
        "    )\n",
        "    return train_loader, val_loader\n",
        "\n",
        "train_loader, val_loader = get_data_loaders(train_batch_size=128, val_batch_size=256)\n",
        "\n",
        "optimizer = torch.optim.RMSprop(model.parameters(), lr=0.005)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "trainer = create_supervised_trainer(model, optimizer, criterion, device)\n",
        "\n",
        "val_metrics = {\n",
        "    \"accuracy\": Accuracy(),\n",
        "    \"nll\": Loss(criterion)\n",
        "}\n",
        "evaluator = create_supervised_evaluator(model, metrics=val_metrics, device=device)\n",
        "\n",
        "# how many batches to wait before logging training status\n",
        "log_interval=20\n",
        "\n",
        "@trainer.on(Events.ITERATION_COMPLETED(every=log_interval))\n",
        "def log_training_loss(engine):\n",
        "    print(f\"Epoch[{engine.state.epoch}] Loss: {engine.state.output:.2f}\")\n",
        "\n",
        "@trainer.on(Events.EPOCH_COMPLETED)\n",
        "def log_training_results(trainer):\n",
        "    evaluator.run(train_loader)\n",
        "    metrics = evaluator.state.metrics\n",
        "    print(f\"Training Results - Epoch[{trainer.state.epoch}] Avg accuracy: {metrics['accuracy']:.2f} Avg loss: {metrics['nll']:.2f}\")\n",
        "\n",
        "@trainer.on(Events.EPOCH_COMPLETED)\n",
        "def log_validation_results(trainer):\n",
        "    evaluator.run(val_loader)\n",
        "    metrics = evaluator.state.metrics\n",
        "    print(f\"Validation Results - Epoch[{trainer.state.epoch}] Avg accuracy: {metrics['accuracy']:.2f} Avg loss: {metrics['nll']:.2f}\")\n",
        "\n",
        "trainer.run(train_loader, max_epochs=5)\n",
        "```"
      ],
      "id": "vya75pqVEcGE"
    }
  ]
}