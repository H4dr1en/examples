{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "hyperparameter-tuning.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "pycharm": {
      "stem_cell": {
        "cell_type": "raw",
        "metadata": {
          "collapsed": false
        },
        "source": []
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5w-QlZE9mvdY"
      },
      "source": [
        "<!-- ---\n",
        "title: Hyperparameter Tuning in Ignite\n",
        "date: 2021-09-28\n",
        "downloads: true\n",
        "sidebar: true\n",
        "tags:\n",
        "  - hyperparameter tuning\n",
        "  - ray tune\n",
        "  - optuna\n",
        "  - ax\n",
        "--- -->\n",
        "\n",
        "#  Hyperparameter Tuning in Ignite"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vJgTaKWU8Doq"
      },
      "source": [
        "In this tutorial, we will see how [Ray Tune](https://docs.ray.io/en/stable/tune.html) can be used with Ignite for hyperparameter tuning. We will also compare it with other frameworks like [Optuna](https://optuna.org/) and [Ax](https://ax.dev/) for hyperparameter optimization.\n",
        "\n",
        "<!--more-->\n",
        "\n",
        "We will follow [this PyTorch tutorial](https://pytorch.org/tutorials/beginner/hyperparameter_tuning_tutorial.html) for reference.\n",
        "\n",
        "In this example, we will be using a [ResNet18](https://pytorch.org/vision/stable/models.html#torchvision.models.resnet18) model on the [MNIST](https://pytorch.org/vision/stable/datasets.html#torchvision.datasets.MNIST) dataset. The base code is the same as used in the [Getting Started Guide](https://pytorch-ignite.ai/tutorials/getting-started/)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qhiy_ylcn2GD"
      },
      "source": [
        "## Required Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1zevsoVQ4nx7"
      },
      "source": [
        "!pip install pytorch-ignite\n",
        "!pip install ray"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TrvIsRKQn42e"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lMphyBmmmvdw",
        "pycharm": {
          "is_executing": false
        }
      },
      "source": [
        "import os\n",
        "from functools import partial\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import random_split\n",
        "\n",
        "from torch.utils.data import DataLoader, SubsetRandomSampler\n",
        "from torchvision.datasets import CIFAR10\n",
        "from torchvision.models import resnet18\n",
        "from torchvision.transforms import Compose, Normalize, ToTensor\n",
        "\n",
        "from ignite.engine import Events, create_supervised_trainer, create_supervised_evaluator\n",
        "from ignite.metrics import Accuracy, Loss, RunningAverage\n",
        "import ignite.distributed as idist\n",
        "\n",
        "from ray import tune\n",
        "from ray.tune import CLIReporter\n",
        "from ray.tune.schedulers import ASHAScheduler"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eZeKOgKymvdx"
      },
      "source": [
        "def load_data(data_dir=\"./data\"):\n",
        "    transform = Compose([\n",
        "        ToTensor(),\n",
        "        Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "    ])\n",
        "\n",
        "    trainset = CIFAR10(\n",
        "        root=data_dir, train=True, download=True, transform=transform)\n",
        "\n",
        "    testset = CIFAR10(\n",
        "        root=data_dir, train=False, download=True, transform=transform)\n",
        "\n",
        "    return trainset, testset"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JLvKsObkH6cF"
      },
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self, l1=120, l2=84):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "        self.fc1 = nn.Linear(16 * 5 * 5, l1)\n",
        "        self.fc2 = nn.Linear(l1, l2)\n",
        "        self.fc3 = nn.Linear(l2, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = x.view(-1, 16 * 5 * 5)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iyrEEGHD9Ie5"
      },
      "source": [
        "def train_cifar(config, checkpoint_dir=None, data_dir=None):\n",
        "    net = idist.auto_model(Net(config[\"l1\"], config[\"l2\"]))\n",
        "\n",
        "    device = idist.device()\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = idist.auto_optim(optim.SGD(net.parameters(), lr=config[\"lr\"], momentum=0.9))\n",
        "\n",
        "    trainer = create_supervised_trainer(net, optimizer, criterion, device=device, non_blocking=True)\n",
        "    val_evaluator = create_supervised_evaluator(net, metrics={ \"accuracy\": Accuracy(), \"loss\": Loss(criterion)}, device=device, non_blocking=True)\n",
        "\n",
        "    to_save = { \"model\": net, \"optimizer\": optimizer}\n",
        "\n",
        "    if checkpoint_dir:\n",
        "        model_state, optimizer_state = torch.load(\n",
        "            os.path.join(checkpoint_dir, \"checkpoint\"))\n",
        "        net.load_state_dict(model_state)\n",
        "        optimizer.load_state_dict(optimizer_state)\n",
        "\n",
        "    trainset, testset = load_data(data_dir)\n",
        "\n",
        "    test_abs = int(len(trainset) * 0.8)\n",
        "    train_subset, val_subset = random_split(\n",
        "        trainset, [test_abs, len(trainset) - test_abs])\n",
        "\n",
        "    trainloader = idist.auto_dataloader(\n",
        "        train_subset,\n",
        "        batch_size=int(config[\"batch_size\"]),\n",
        "        shuffle=True,\n",
        "        num_workers=8)\n",
        "    valloader = idist.auto_dataloader(\n",
        "        val_subset,\n",
        "        batch_size=int(config[\"batch_size\"]),\n",
        "        shuffle=True,\n",
        "        num_workers=8)\n",
        "\n",
        "    avg_output = RunningAverage(output_transform=lambda x: x)\n",
        "    avg_output.attach(trainer, 'running_avg_loss')\n",
        "\n",
        "    # handler = Checkpoint(\n",
        "    #     to_save, DiskSaver('models', create_dir=True), n_saved=2, global_step_transform=gst\n",
        "    # )\n",
        "    # trainer.add_event_handler(Events.EPOCH_COMPLETED, handler)\n",
        "\n",
        "    @trainer.on(Events.ITERATION_COMPLETED(every=2000))\n",
        "    def log_training_loss(engine):\n",
        "        print(f\"Epoch[{engine.state.epoch}], Iter[{engine.state.iteration}] Loss: {engine.state.output:.2f} Running Avg Loss: {engine.state.metrics['running_avg_loss']:.2f}\")\n",
        "\n",
        "\n",
        "    @trainer.on(Events.EPOCH_COMPLETED)\n",
        "    def log_validation_results(trainer):\n",
        "        val_evaluator.run(valloader)\n",
        "        metrics = val_evaluator.state.metrics\n",
        "        print(f\"Validation Results - Epoch[{trainer.state.epoch}] Avg accuracy: {metrics['accuracy']:.2f} Avg loss: {metrics['loss']:.2f}\")\n",
        "\n",
        "        with tune.checkpoint_dir(trainer.state.epoch) as checkpoint_dir:\n",
        "          path = os.path.join(checkpoint_dir, \"checkpoint\")\n",
        "          torch.save((net.state_dict(), optimizer.state_dict()), path)\n",
        "        tune.report(loss=metrics['loss'], accuracy=metrics['accuracy'])   \n",
        "\n",
        "    trainer.run(trainloader, max_epochs=10) \n",
        "\n",
        "        \n",
        "    print(\"Finished Training\")"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aZkK8AqIApaR"
      },
      "source": [
        "def main(num_samples=10, max_num_epochs=10, gpus_per_trial=1):\n",
        "    data_dir = os.path.abspath(\"./data\")\n",
        "    trainset, testset = load_data(data_dir)\n",
        "    config = {\n",
        "        \"l1\": tune.sample_from(lambda _: 2 ** np.random.randint(2, 9)),\n",
        "        \"l2\": tune.sample_from(lambda _: 2 ** np.random.randint(2, 9)),\n",
        "        \"lr\": tune.loguniform(1e-4, 1e-1),\n",
        "        \"batch_size\": tune.choice([2, 4, 8, 16])\n",
        "    }\n",
        "    scheduler = ASHAScheduler(\n",
        "        metric=\"loss\",\n",
        "        mode=\"min\",\n",
        "        max_t=max_num_epochs,\n",
        "        grace_period=1,\n",
        "        reduction_factor=2)\n",
        "    reporter = CLIReporter(\n",
        "        # parameter_columns=[\"l1\", \"l2\", \"lr\", \"batch_size\"],\n",
        "        metric_columns=[\"loss\", \"accuracy\", \"training_iteration\"])\n",
        "    result = tune.run(\n",
        "        partial(train_cifar, data_dir=data_dir),\n",
        "        resources_per_trial={\"cpu\": 2, \"gpu\": gpus_per_trial},\n",
        "        config=config,\n",
        "        num_samples=num_samples,\n",
        "        scheduler=scheduler,\n",
        "        progress_reporter=reporter)\n",
        "\n",
        "    best_trial = result.get_best_trial(\"loss\", \"min\", \"last\")\n",
        "    print(\"Best trial config: {}\".format(best_trial.config))\n",
        "    print(\"Best trial final validation loss: {}\".format(\n",
        "        best_trial.last_result[\"loss\"]))\n",
        "    print(\"Best trial final validation accuracy: {}\".format(\n",
        "        best_trial.last_result[\"accuracy\"]))\n",
        "\n",
        "    best_trained_model = idist.auto_model(Net(best_trial.config[\"l1\"], best_trial.config[\"l2\"]))\n",
        "    device = idist.device()\n",
        "\n",
        "    best_checkpoint_dir = best_trial.checkpoint.value\n",
        "    model_state, optimizer_state = torch.load(os.path.join(\n",
        "        best_checkpoint_dir, \"checkpoint\"))\n",
        "    best_trained_model.load_state_dict(model_state)\n",
        "\n",
        "    test_evaluator = create_supervised_evaluator(best_trained_model, metrics={\"Accuracy\": Accuracy()}, device=device, non_blocking=True)\n",
        "\n",
        "    trainset, testset = load_data()\n",
        "\n",
        "    testloader = idist.auto_dataloader(testset, batch_size=4, shuffle=False, num_workers=2)\n",
        "\n",
        "    test_evaluator.run(testloader)\n",
        "    print(test_evaluator.state.metrics)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1LV99-55D5Go",
        "outputId": "983cd5d7-bb9c-4fec-d97f-e4ec942b8625"
      },
      "source": [
        "main(num_samples=3, max_num_epochs=3, gpus_per_trial=1)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2021-10-01 11:50:00,377\tWARNING experiment.py:296 -- No name detected on trainable. Using DEFAULT.\n",
            "2021-10-01 11:50:00,379\tINFO registry.py:67 -- Detected unknown callable for trainable. Converting to class.\n",
            "2021-10-01 11:50:00,396\tWARNING callback.py:117 -- The TensorboardX logger cannot be instantiated because either TensorboardX or one of it's dependencies is not installed. Please make sure you have the latest version of TensorboardX installed: `pip install -U tensorboardx`\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Status ==\n",
            "Memory usage on this node: 2.2/12.7 GiB\n",
            "Using AsyncHyperBand: num_stopped=0\n",
            "Bracket: Iter 2.000: None | Iter 1.000: None\n",
            "Resources requested: 0/2 CPUs, 0/1 GPUs, 0.0/7.33 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:K80)\n",
            "Result logdir: /root/ray_results/DEFAULT_2021-10-01_11-50-00\n",
            "Number of trials: 3/3 (3 PENDING)\n",
            "+---------------------+----------+-------+--------------+------+------+------------+\n",
            "| Trial name          | status   | loc   |   batch_size |   l1 |   l2 |         lr |\n",
            "|---------------------+----------+-------+--------------+------+------+------------|\n",
            "| DEFAULT_b50ea_00000 | PENDING  |       |            8 |   16 |    4 | 0.00059636 |\n",
            "| DEFAULT_b50ea_00001 | PENDING  |       |            2 |   32 |   64 | 0.00074678 |\n",
            "| DEFAULT_b50ea_00002 | PENDING  |       |            2 |   64 |   32 | 0.00122324 |\n",
            "+---------------------+----------+-------+--------------+------+------+------------+\n",
            "\n",
            "\n",
            "\u001b[2m\u001b[36m(pid=1853)\u001b[0m Files already downloaded and verified\n",
            "== Status ==\n",
            "Memory usage on this node: 3.2/12.7 GiB\n",
            "Using AsyncHyperBand: num_stopped=0\n",
            "Bracket: Iter 2.000: None | Iter 1.000: None\n",
            "Resources requested: 2.0/2 CPUs, 1.0/1 GPUs, 0.0/7.33 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:K80)\n",
            "Result logdir: /root/ray_results/DEFAULT_2021-10-01_11-50-00\n",
            "Number of trials: 3/3 (2 PENDING, 1 RUNNING)\n",
            "+---------------------+----------+-------+--------------+------+------+------------+\n",
            "| Trial name          | status   | loc   |   batch_size |   l1 |   l2 |         lr |\n",
            "|---------------------+----------+-------+--------------+------+------+------------|\n",
            "| DEFAULT_b50ea_00000 | RUNNING  |       |            8 |   16 |    4 | 0.00059636 |\n",
            "| DEFAULT_b50ea_00001 | PENDING  |       |            2 |   32 |   64 | 0.00074678 |\n",
            "| DEFAULT_b50ea_00002 | PENDING  |       |            2 |   64 |   32 | 0.00122324 |\n",
            "+---------------------+----------+-------+--------------+------+------+------------+\n",
            "\n",
            "\n",
            "\u001b[2m\u001b[36m(pid=1853)\u001b[0m Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(pid=1853)\u001b[0m 2021-10-01 11:50:06,682 ignite.distributed.auto.auto_dataloader INFO: Use data loader kwargs for dataset '<torch.utils.data.da': \n",
            "\u001b[2m\u001b[36m(pid=1853)\u001b[0m \t{'batch_size': 8, 'shuffle': True, 'num_workers': 8, 'pin_memory': True}\n",
            "\u001b[2m\u001b[36m(pid=1853)\u001b[0m /usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "\u001b[2m\u001b[36m(pid=1853)\u001b[0m   cpuset_checked))\n",
            "\u001b[2m\u001b[36m(pid=1853)\u001b[0m 2021-10-01 11:50:06,683 ignite.distributed.auto.auto_dataloader INFO: Use data loader kwargs for dataset '<torch.utils.data.da': \n",
            "\u001b[2m\u001b[36m(pid=1853)\u001b[0m \t{'batch_size': 8, 'shuffle': True, 'num_workers': 8, 'pin_memory': True}\n",
            "\u001b[2m\u001b[36m(pid=1853)\u001b[0m /usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
            "\u001b[2m\u001b[36m(pid=1853)\u001b[0m   return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2m\u001b[36m(pid=1853)\u001b[0m Epoch[1], Iter[2000] Loss: 2.31 Running Avg Loss: 2.31\n",
            "\u001b[2m\u001b[36m(pid=1853)\u001b[0m Epoch[1], Iter[4000] Loss: 2.31 Running Avg Loss: 2.30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(pid=1853)\u001b[0m [W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "\u001b[2m\u001b[36m(pid=1853)\u001b[0m [W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "\u001b[2m\u001b[36m(pid=1853)\u001b[0m [W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "\u001b[2m\u001b[36m(pid=1853)\u001b[0m [W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "\u001b[2m\u001b[36m(pid=1853)\u001b[0m [W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "\u001b[2m\u001b[36m(pid=1853)\u001b[0m [W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "\u001b[2m\u001b[36m(pid=1853)\u001b[0m [W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "\u001b[2m\u001b[36m(pid=1853)\u001b[0m [W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for DEFAULT_b50ea_00000:\n",
            "  accuracy: 0.1312\n",
            "  date: 2021-10-01_11-50-46\n",
            "  done: false\n",
            "  experiment_id: e32295dd189a4a47a6018a23aa66c38e\n",
            "  hostname: ac982acc27c2\n",
            "  iterations_since_restore: 1\n",
            "  loss: 2.3001421875\n",
            "  node_ip: 172.28.0.2\n",
            "  pid: 1853\n",
            "  should_checkpoint: true\n",
            "  time_since_restore: 44.10174560546875\n",
            "  time_this_iter_s: 44.10174560546875\n",
            "  time_total_s: 44.10174560546875\n",
            "  timestamp: 1633089046\n",
            "  timesteps_since_restore: 0\n",
            "  training_iteration: 1\n",
            "  trial_id: b50ea_00000\n",
            "  \n",
            "== Status ==\n",
            "Memory usage on this node: 3.4/12.7 GiB\n",
            "Using AsyncHyperBand: num_stopped=0\n",
            "Bracket: Iter 2.000: None | Iter 1.000: -2.3001421875\n",
            "Resources requested: 2.0/2 CPUs, 1.0/1 GPUs, 0.0/7.33 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:K80)\n",
            "Result logdir: /root/ray_results/DEFAULT_2021-10-01_11-50-00\n",
            "Number of trials: 3/3 (2 PENDING, 1 RUNNING)\n",
            "+---------------------+----------+-----------------+--------------+------+------+------------+---------+------------+----------------------+\n",
            "| Trial name          | status   | loc             |   batch_size |   l1 |   l2 |         lr |    loss |   accuracy |   training_iteration |\n",
            "|---------------------+----------+-----------------+--------------+------+------+------------+---------+------------+----------------------|\n",
            "| DEFAULT_b50ea_00000 | RUNNING  | 172.28.0.2:1853 |            8 |   16 |    4 | 0.00059636 | 2.30014 |     0.1312 |                    1 |\n",
            "| DEFAULT_b50ea_00001 | PENDING  |                 |            2 |   32 |   64 | 0.00074678 |         |            |                      |\n",
            "| DEFAULT_b50ea_00002 | PENDING  |                 |            2 |   64 |   32 | 0.00122324 |         |            |                      |\n",
            "+---------------------+----------+-----------------+--------------+------+------+------------+---------+------------+----------------------+\n",
            "\n",
            "\n",
            "\u001b[2m\u001b[36m(pid=1853)\u001b[0m Validation Results - Epoch[1] Avg accuracy: 0.13 Avg loss: 2.30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(pid=1853)\u001b[0m [W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "\u001b[2m\u001b[36m(pid=1853)\u001b[0m [W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "\u001b[2m\u001b[36m(pid=1853)\u001b[0m [W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "\u001b[2m\u001b[36m(pid=1853)\u001b[0m [W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "\u001b[2m\u001b[36m(pid=1853)\u001b[0m [W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "\u001b[2m\u001b[36m(pid=1853)\u001b[0m [W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "\u001b[2m\u001b[36m(pid=1853)\u001b[0m [W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "\u001b[2m\u001b[36m(pid=1853)\u001b[0m [W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2m\u001b[36m(pid=1853)\u001b[0m Epoch[2], Iter[6000] Loss: 2.30 Running Avg Loss: 2.29\n",
            "\u001b[2m\u001b[36m(pid=1853)\u001b[0m Epoch[2], Iter[8000] Loss: 2.26 Running Avg Loss: 2.21\n",
            "\u001b[2m\u001b[36m(pid=1853)\u001b[0m Epoch[2], Iter[10000] Loss: 2.52 Running Avg Loss: 2.08\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(pid=1853)\u001b[0m [W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "\u001b[2m\u001b[36m(pid=1853)\u001b[0m [W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "\u001b[2m\u001b[36m(pid=1853)\u001b[0m [W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "\u001b[2m\u001b[36m(pid=1853)\u001b[0m [W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "\u001b[2m\u001b[36m(pid=1853)\u001b[0m [W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "\u001b[2m\u001b[36m(pid=1853)\u001b[0m [W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "\u001b[2m\u001b[36m(pid=1853)\u001b[0m [W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "\u001b[2m\u001b[36m(pid=1853)\u001b[0m [W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for DEFAULT_b50ea_00000:\n",
            "  accuracy: 0.2316\n",
            "  date: 2021-10-01_11-51-27\n",
            "  done: false\n",
            "  experiment_id: e32295dd189a4a47a6018a23aa66c38e\n",
            "  hostname: ac982acc27c2\n",
            "  iterations_since_restore: 2\n",
            "  loss: 2.0590619140625\n",
            "  node_ip: 172.28.0.2\n",
            "  pid: 1853\n",
            "  should_checkpoint: true\n",
            "  time_since_restore: 84.69665455818176\n",
            "  time_this_iter_s: 40.59490895271301\n",
            "  time_total_s: 84.69665455818176\n",
            "  timestamp: 1633089087\n",
            "  timesteps_since_restore: 0\n",
            "  training_iteration: 2\n",
            "  trial_id: b50ea_00000\n",
            "  \n",
            "== Status ==\n",
            "Memory usage on this node: 3.4/12.7 GiB\n",
            "Using AsyncHyperBand: num_stopped=0\n",
            "Bracket: Iter 2.000: -2.0590619140625 | Iter 1.000: -2.3001421875\n",
            "Resources requested: 2.0/2 CPUs, 1.0/1 GPUs, 0.0/7.33 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:K80)\n",
            "Result logdir: /root/ray_results/DEFAULT_2021-10-01_11-50-00\n",
            "Number of trials: 3/3 (2 PENDING, 1 RUNNING)\n",
            "+---------------------+----------+-----------------+--------------+------+------+------------+---------+------------+----------------------+\n",
            "| Trial name          | status   | loc             |   batch_size |   l1 |   l2 |         lr |    loss |   accuracy |   training_iteration |\n",
            "|---------------------+----------+-----------------+--------------+------+------+------------+---------+------------+----------------------|\n",
            "| DEFAULT_b50ea_00000 | RUNNING  | 172.28.0.2:1853 |            8 |   16 |    4 | 0.00059636 | 2.05906 |     0.2316 |                    2 |\n",
            "| DEFAULT_b50ea_00001 | PENDING  |                 |            2 |   32 |   64 | 0.00074678 |         |            |                      |\n",
            "| DEFAULT_b50ea_00002 | PENDING  |                 |            2 |   64 |   32 | 0.00122324 |         |            |                      |\n",
            "+---------------------+----------+-----------------+--------------+------+------+------------+---------+------------+----------------------+\n",
            "\n",
            "\n",
            "\u001b[2m\u001b[36m(pid=1853)\u001b[0m Validation Results - Epoch[2] Avg accuracy: 0.23 Avg loss: 2.06\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(pid=1853)\u001b[0m [W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "\u001b[2m\u001b[36m(pid=1853)\u001b[0m [W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "\u001b[2m\u001b[36m(pid=1853)\u001b[0m [W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "\u001b[2m\u001b[36m(pid=1853)\u001b[0m [W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "\u001b[2m\u001b[36m(pid=1853)\u001b[0m [W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "\u001b[2m\u001b[36m(pid=1853)\u001b[0m [W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "\u001b[2m\u001b[36m(pid=1853)\u001b[0m [W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "\u001b[2m\u001b[36m(pid=1853)\u001b[0m [W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2m\u001b[36m(pid=1853)\u001b[0m Epoch[3], Iter[12000] Loss: 1.76 Running Avg Loss: 1.82\n",
            "\u001b[2m\u001b[36m(pid=1853)\u001b[0m Epoch[3], Iter[14000] Loss: 1.76 Running Avg Loss: 1.83\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(pid=1853)\u001b[0m [W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "\u001b[2m\u001b[36m(pid=1853)\u001b[0m [W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "\u001b[2m\u001b[36m(pid=1853)\u001b[0m [W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "\u001b[2m\u001b[36m(pid=1853)\u001b[0m [W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "\u001b[2m\u001b[36m(pid=1853)\u001b[0m [W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "\u001b[2m\u001b[36m(pid=1853)\u001b[0m [W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "\u001b[2m\u001b[36m(pid=1853)\u001b[0m [W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "\u001b[2m\u001b[36m(pid=1853)\u001b[0m [W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for DEFAULT_b50ea_00000:\n",
            "  accuracy: 0.3351\n",
            "  date: 2021-10-01_11-52-08\n",
            "  done: true\n",
            "  experiment_id: e32295dd189a4a47a6018a23aa66c38e\n",
            "  hostname: ac982acc27c2\n",
            "  iterations_since_restore: 3\n",
            "  loss: 1.724516796875\n",
            "  node_ip: 172.28.0.2\n",
            "  pid: 1853\n",
            "  should_checkpoint: true\n",
            "  time_since_restore: 125.52060723304749\n",
            "  time_this_iter_s: 40.82395267486572\n",
            "  time_total_s: 125.52060723304749\n",
            "  timestamp: 1633089128\n",
            "  timesteps_since_restore: 0\n",
            "  training_iteration: 3\n",
            "  trial_id: b50ea_00000\n",
            "  \n",
            "== Status ==\n",
            "Memory usage on this node: 3.4/12.7 GiB\n",
            "Using AsyncHyperBand: num_stopped=1\n",
            "Bracket: Iter 2.000: -2.0590619140625 | Iter 1.000: -2.3001421875\n",
            "Resources requested: 2.0/2 CPUs, 1.0/1 GPUs, 0.0/7.33 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:K80)\n",
            "Result logdir: /root/ray_results/DEFAULT_2021-10-01_11-50-00\n",
            "Number of trials: 3/3 (2 PENDING, 1 RUNNING)\n",
            "+---------------------+----------+-----------------+--------------+------+------+------------+---------+------------+----------------------+\n",
            "| Trial name          | status   | loc             |   batch_size |   l1 |   l2 |         lr |    loss |   accuracy |   training_iteration |\n",
            "|---------------------+----------+-----------------+--------------+------+------+------------+---------+------------+----------------------|\n",
            "| DEFAULT_b50ea_00000 | RUNNING  | 172.28.0.2:1853 |            8 |   16 |    4 | 0.00059636 | 1.72452 |     0.3351 |                    3 |\n",
            "| DEFAULT_b50ea_00001 | PENDING  |                 |            2 |   32 |   64 | 0.00074678 |         |            |                      |\n",
            "| DEFAULT_b50ea_00002 | PENDING  |                 |            2 |   64 |   32 | 0.00122324 |         |            |                      |\n",
            "+---------------------+----------+-----------------+--------------+------+------+------------+---------+------------+----------------------+\n",
            "\n",
            "\n",
            "\u001b[2m\u001b[36m(pid=1853)\u001b[0m Validation Results - Epoch[3] Avg accuracy: 0.34 Avg loss: 1.72\n",
            "\u001b[2m\u001b[36m(pid=2098)\u001b[0m Files already downloaded and verified\n",
            "\u001b[2m\u001b[36m(pid=2098)\u001b[0m Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(pid=2098)\u001b[0m 2021-10-01 11:52:14,024 ignite.distributed.auto.auto_dataloader INFO: Use data loader kwargs for dataset '<torch.utils.data.da': \n",
            "\u001b[2m\u001b[36m(pid=2098)\u001b[0m \t{'batch_size': 2, 'shuffle': True, 'num_workers': 8, 'pin_memory': True}\n",
            "\u001b[2m\u001b[36m(pid=2098)\u001b[0m /usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "\u001b[2m\u001b[36m(pid=2098)\u001b[0m   cpuset_checked))\n",
            "\u001b[2m\u001b[36m(pid=2098)\u001b[0m 2021-10-01 11:52:14,025 ignite.distributed.auto.auto_dataloader INFO: Use data loader kwargs for dataset '<torch.utils.data.da': \n",
            "\u001b[2m\u001b[36m(pid=2098)\u001b[0m \t{'batch_size': 2, 'shuffle': True, 'num_workers': 8, 'pin_memory': True}\n",
            "\u001b[2m\u001b[36m(pid=2098)\u001b[0m /usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
            "\u001b[2m\u001b[36m(pid=2098)\u001b[0m   return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2m\u001b[36m(pid=2098)\u001b[0m Epoch[1], Iter[2000] Loss: 2.23 Running Avg Loss: 2.19\n",
            "\u001b[2m\u001b[36m(pid=2098)\u001b[0m Epoch[1], Iter[4000] Loss: 1.13 Running Avg Loss: 1.89\n",
            "\u001b[2m\u001b[36m(pid=2098)\u001b[0m Epoch[1], Iter[6000] Loss: 1.89 Running Avg Loss: 1.75\n",
            "\u001b[2m\u001b[36m(pid=2098)\u001b[0m Epoch[1], Iter[8000] Loss: 2.85 Running Avg Loss: 1.65\n",
            "\u001b[2m\u001b[36m(pid=2098)\u001b[0m Epoch[1], Iter[10000] Loss: 1.28 Running Avg Loss: 1.58\n",
            "\u001b[2m\u001b[36m(pid=2098)\u001b[0m Epoch[1], Iter[12000] Loss: 3.34 Running Avg Loss: 1.71\n",
            "\u001b[2m\u001b[36m(pid=2098)\u001b[0m Epoch[1], Iter[14000] Loss: 1.81 Running Avg Loss: 1.57\n",
            "\u001b[2m\u001b[36m(pid=2098)\u001b[0m Epoch[1], Iter[16000] Loss: 1.26 Running Avg Loss: 1.40\n",
            "\u001b[2m\u001b[36m(pid=2098)\u001b[0m Epoch[1], Iter[18000] Loss: 0.86 Running Avg Loss: 1.56\n",
            "\u001b[2m\u001b[36m(pid=2098)\u001b[0m Epoch[1], Iter[20000] Loss: 1.49 Running Avg Loss: 1.38\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(pid=2098)\u001b[0m [W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "\u001b[2m\u001b[36m(pid=2098)\u001b[0m [W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "\u001b[2m\u001b[36m(pid=2098)\u001b[0m [W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "\u001b[2m\u001b[36m(pid=2098)\u001b[0m [W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "\u001b[2m\u001b[36m(pid=2098)\u001b[0m [W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "\u001b[2m\u001b[36m(pid=2098)\u001b[0m [W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "\u001b[2m\u001b[36m(pid=2098)\u001b[0m [W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "\u001b[2m\u001b[36m(pid=2098)\u001b[0m [W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for DEFAULT_b50ea_00001:\n",
            "  accuracy: 0.4689\n",
            "  date: 2021-10-01_11-54-20\n",
            "  done: false\n",
            "  experiment_id: 036f604837ca47d3bd6fde124228b865\n",
            "  hostname: ac982acc27c2\n",
            "  iterations_since_restore: 1\n",
            "  loss: 1.45748916015625\n",
            "  node_ip: 172.28.0.2\n",
            "  pid: 2098\n",
            "  should_checkpoint: true\n",
            "  time_since_restore: 130.6066439151764\n",
            "  time_this_iter_s: 130.6066439151764\n",
            "  time_total_s: 130.6066439151764\n",
            "  timestamp: 1633089260\n",
            "  timesteps_since_restore: 0\n",
            "  training_iteration: 1\n",
            "  trial_id: b50ea_00001\n",
            "  \n",
            "== Status ==\n",
            "Memory usage on this node: 3.5/12.7 GiB\n",
            "Using AsyncHyperBand: num_stopped=1\n",
            "Bracket: Iter 2.000: -2.0590619140625 | Iter 1.000: -1.8788156738281252\n",
            "Resources requested: 2.0/2 CPUs, 1.0/1 GPUs, 0.0/7.33 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:K80)\n",
            "Result logdir: /root/ray_results/DEFAULT_2021-10-01_11-50-00\n",
            "Number of trials: 3/3 (1 PENDING, 1 RUNNING, 1 TERMINATED)\n",
            "+---------------------+------------+-----------------+--------------+------+------+------------+---------+------------+----------------------+\n",
            "| Trial name          | status     | loc             |   batch_size |   l1 |   l2 |         lr |    loss |   accuracy |   training_iteration |\n",
            "|---------------------+------------+-----------------+--------------+------+------+------------+---------+------------+----------------------|\n",
            "| DEFAULT_b50ea_00001 | RUNNING    | 172.28.0.2:2098 |            2 |   32 |   64 | 0.00074678 | 1.45749 |     0.4689 |                    1 |\n",
            "| DEFAULT_b50ea_00002 | PENDING    |                 |            2 |   64 |   32 | 0.00122324 |         |            |                      |\n",
            "| DEFAULT_b50ea_00000 | TERMINATED |                 |            8 |   16 |    4 | 0.00059636 | 1.72452 |     0.3351 |                    3 |\n",
            "+---------------------+------------+-----------------+--------------+------+------+------------+---------+------------+----------------------+\n",
            "\n",
            "\n",
            "\u001b[2m\u001b[36m(pid=2098)\u001b[0m Validation Results - Epoch[1] Avg accuracy: 0.47 Avg loss: 1.46\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(pid=2098)\u001b[0m [W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "\u001b[2m\u001b[36m(pid=2098)\u001b[0m [W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "\u001b[2m\u001b[36m(pid=2098)\u001b[0m [W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "\u001b[2m\u001b[36m(pid=2098)\u001b[0m [W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "\u001b[2m\u001b[36m(pid=2098)\u001b[0m [W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "\u001b[2m\u001b[36m(pid=2098)\u001b[0m [W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "\u001b[2m\u001b[36m(pid=2098)\u001b[0m [W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "\u001b[2m\u001b[36m(pid=2098)\u001b[0m [W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2m\u001b[36m(pid=2098)\u001b[0m Epoch[2], Iter[22000] Loss: 1.37 Running Avg Loss: 1.46\n",
            "\u001b[2m\u001b[36m(pid=2098)\u001b[0m Epoch[2], Iter[24000] Loss: 0.96 Running Avg Loss: 1.35\n",
            "\u001b[2m\u001b[36m(pid=2098)\u001b[0m Epoch[2], Iter[26000] Loss: 1.13 Running Avg Loss: 1.40\n",
            "\u001b[2m\u001b[36m(pid=2098)\u001b[0m Epoch[2], Iter[28000] Loss: 1.63 Running Avg Loss: 1.45\n",
            "\u001b[2m\u001b[36m(pid=2098)\u001b[0m Epoch[2], Iter[30000] Loss: 1.68 Running Avg Loss: 1.51\n",
            "\u001b[2m\u001b[36m(pid=2098)\u001b[0m Epoch[2], Iter[32000] Loss: 2.97 Running Avg Loss: 1.41\n",
            "\u001b[2m\u001b[36m(pid=2098)\u001b[0m Epoch[2], Iter[34000] Loss: 1.36 Running Avg Loss: 1.42\n",
            "\u001b[2m\u001b[36m(pid=2098)\u001b[0m Epoch[2], Iter[36000] Loss: 2.18 Running Avg Loss: 1.39\n",
            "\u001b[2m\u001b[36m(pid=2098)\u001b[0m Epoch[2], Iter[38000] Loss: 1.48 Running Avg Loss: 1.22\n",
            "\u001b[2m\u001b[36m(pid=2098)\u001b[0m Epoch[2], Iter[40000] Loss: 1.10 Running Avg Loss: 1.29\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(pid=2098)\u001b[0m [W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "\u001b[2m\u001b[36m(pid=2098)\u001b[0m [W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "\u001b[2m\u001b[36m(pid=2098)\u001b[0m [W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "\u001b[2m\u001b[36m(pid=2098)\u001b[0m [W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "\u001b[2m\u001b[36m(pid=2098)\u001b[0m [W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "\u001b[2m\u001b[36m(pid=2098)\u001b[0m [W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "\u001b[2m\u001b[36m(pid=2098)\u001b[0m [W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "\u001b[2m\u001b[36m(pid=2098)\u001b[0m [W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for DEFAULT_b50ea_00001:\n",
            "  accuracy: 0.5089\n",
            "  date: 2021-10-01_11-56-27\n",
            "  done: false\n",
            "  experiment_id: 036f604837ca47d3bd6fde124228b865\n",
            "  hostname: ac982acc27c2\n",
            "  iterations_since_restore: 2\n",
            "  loss: 1.36539609375\n",
            "  node_ip: 172.28.0.2\n",
            "  pid: 2098\n",
            "  should_checkpoint: true\n",
            "  time_since_restore: 257.20395398139954\n",
            "  time_this_iter_s: 126.59731006622314\n",
            "  time_total_s: 257.20395398139954\n",
            "  timestamp: 1633089387\n",
            "  timesteps_since_restore: 0\n",
            "  training_iteration: 2\n",
            "  trial_id: b50ea_00001\n",
            "  \n",
            "== Status ==\n",
            "Memory usage on this node: 3.5/12.7 GiB\n",
            "Using AsyncHyperBand: num_stopped=1\n",
            "Bracket: Iter 2.000: -1.7122290039062502 | Iter 1.000: -1.8788156738281252\n",
            "Resources requested: 2.0/2 CPUs, 1.0/1 GPUs, 0.0/7.33 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:K80)\n",
            "Result logdir: /root/ray_results/DEFAULT_2021-10-01_11-50-00\n",
            "Number of trials: 3/3 (1 PENDING, 1 RUNNING, 1 TERMINATED)\n",
            "+---------------------+------------+-----------------+--------------+------+------+------------+---------+------------+----------------------+\n",
            "| Trial name          | status     | loc             |   batch_size |   l1 |   l2 |         lr |    loss |   accuracy |   training_iteration |\n",
            "|---------------------+------------+-----------------+--------------+------+------+------------+---------+------------+----------------------|\n",
            "| DEFAULT_b50ea_00001 | RUNNING    | 172.28.0.2:2098 |            2 |   32 |   64 | 0.00074678 | 1.3654  |     0.5089 |                    2 |\n",
            "| DEFAULT_b50ea_00002 | PENDING    |                 |            2 |   64 |   32 | 0.00122324 |         |            |                      |\n",
            "| DEFAULT_b50ea_00000 | TERMINATED |                 |            8 |   16 |    4 | 0.00059636 | 1.72452 |     0.3351 |                    3 |\n",
            "+---------------------+------------+-----------------+--------------+------+------+------------+---------+------------+----------------------+\n",
            "\n",
            "\n",
            "\u001b[2m\u001b[36m(pid=2098)\u001b[0m Validation Results - Epoch[2] Avg accuracy: 0.51 Avg loss: 1.37\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(pid=2098)\u001b[0m [W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "\u001b[2m\u001b[36m(pid=2098)\u001b[0m [W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "\u001b[2m\u001b[36m(pid=2098)\u001b[0m [W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "\u001b[2m\u001b[36m(pid=2098)\u001b[0m [W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "\u001b[2m\u001b[36m(pid=2098)\u001b[0m [W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "\u001b[2m\u001b[36m(pid=2098)\u001b[0m [W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "\u001b[2m\u001b[36m(pid=2098)\u001b[0m [W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "\u001b[2m\u001b[36m(pid=2098)\u001b[0m [W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2m\u001b[36m(pid=2098)\u001b[0m Epoch[3], Iter[42000] Loss: 2.02 Running Avg Loss: 1.34\n",
            "\u001b[2m\u001b[36m(pid=2098)\u001b[0m Epoch[3], Iter[44000] Loss: 1.41 Running Avg Loss: 1.26\n",
            "\u001b[2m\u001b[36m(pid=2098)\u001b[0m Epoch[3], Iter[46000] Loss: 0.31 Running Avg Loss: 1.31\n",
            "\u001b[2m\u001b[36m(pid=2098)\u001b[0m Epoch[3], Iter[48000] Loss: 1.21 Running Avg Loss: 1.43\n",
            "\u001b[2m\u001b[36m(pid=2098)\u001b[0m Epoch[3], Iter[50000] Loss: 1.30 Running Avg Loss: 1.40\n",
            "\u001b[2m\u001b[36m(pid=2098)\u001b[0m Epoch[3], Iter[52000] Loss: 0.17 Running Avg Loss: 1.29\n",
            "\u001b[2m\u001b[36m(pid=2098)\u001b[0m Epoch[3], Iter[54000] Loss: 1.23 Running Avg Loss: 1.21\n",
            "\u001b[2m\u001b[36m(pid=2098)\u001b[0m Epoch[3], Iter[56000] Loss: 0.61 Running Avg Loss: 1.47\n",
            "\u001b[2m\u001b[36m(pid=2098)\u001b[0m Epoch[3], Iter[58000] Loss: 1.67 Running Avg Loss: 1.35\n",
            "\u001b[2m\u001b[36m(pid=2098)\u001b[0m Epoch[3], Iter[60000] Loss: 1.83 Running Avg Loss: 1.30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(pid=2098)\u001b[0m [W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "\u001b[2m\u001b[36m(pid=2098)\u001b[0m [W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "\u001b[2m\u001b[36m(pid=2098)\u001b[0m [W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "\u001b[2m\u001b[36m(pid=2098)\u001b[0m [W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "\u001b[2m\u001b[36m(pid=2098)\u001b[0m [W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "\u001b[2m\u001b[36m(pid=2098)\u001b[0m [W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "\u001b[2m\u001b[36m(pid=2098)\u001b[0m [W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "\u001b[2m\u001b[36m(pid=2098)\u001b[0m [W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for DEFAULT_b50ea_00001:\n",
            "  accuracy: 0.5269\n",
            "  date: 2021-10-01_11-58-34\n",
            "  done: true\n",
            "  experiment_id: 036f604837ca47d3bd6fde124228b865\n",
            "  hostname: ac982acc27c2\n",
            "  iterations_since_restore: 3\n",
            "  loss: 1.315916015625\n",
            "  node_ip: 172.28.0.2\n",
            "  pid: 2098\n",
            "  should_checkpoint: true\n",
            "  time_since_restore: 384.62691617012024\n",
            "  time_this_iter_s: 127.4229621887207\n",
            "  time_total_s: 384.62691617012024\n",
            "  timestamp: 1633089514\n",
            "  timesteps_since_restore: 0\n",
            "  training_iteration: 3\n",
            "  trial_id: b50ea_00001\n",
            "  \n",
            "== Status ==\n",
            "Memory usage on this node: 3.4/12.7 GiB\n",
            "Using AsyncHyperBand: num_stopped=2\n",
            "Bracket: Iter 2.000: -1.7122290039062502 | Iter 1.000: -1.8788156738281252\n",
            "Resources requested: 2.0/2 CPUs, 1.0/1 GPUs, 0.0/7.33 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:K80)\n",
            "Result logdir: /root/ray_results/DEFAULT_2021-10-01_11-50-00\n",
            "Number of trials: 3/3 (1 PENDING, 1 RUNNING, 1 TERMINATED)\n",
            "+---------------------+------------+-----------------+--------------+------+------+------------+---------+------------+----------------------+\n",
            "| Trial name          | status     | loc             |   batch_size |   l1 |   l2 |         lr |    loss |   accuracy |   training_iteration |\n",
            "|---------------------+------------+-----------------+--------------+------+------+------------+---------+------------+----------------------|\n",
            "| DEFAULT_b50ea_00001 | RUNNING    | 172.28.0.2:2098 |            2 |   32 |   64 | 0.00074678 | 1.31592 |     0.5269 |                    3 |\n",
            "| DEFAULT_b50ea_00002 | PENDING    |                 |            2 |   64 |   32 | 0.00122324 |         |            |                      |\n",
            "| DEFAULT_b50ea_00000 | TERMINATED |                 |            8 |   16 |    4 | 0.00059636 | 1.72452 |     0.3351 |                    3 |\n",
            "+---------------------+------------+-----------------+--------------+------+------+------------+---------+------------+----------------------+\n",
            "\n",
            "\n",
            "\u001b[2m\u001b[36m(pid=2098)\u001b[0m Validation Results - Epoch[3] Avg accuracy: 0.53 Avg loss: 1.32\n",
            "\u001b[2m\u001b[36m(pid=2361)\u001b[0m Files already downloaded and verified\n",
            "\u001b[2m\u001b[36m(pid=2361)\u001b[0m Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(pid=2361)\u001b[0m 2021-10-01 11:58:40,529 ignite.distributed.auto.auto_dataloader INFO: Use data loader kwargs for dataset '<torch.utils.data.da': \n",
            "\u001b[2m\u001b[36m(pid=2361)\u001b[0m \t{'batch_size': 2, 'shuffle': True, 'num_workers': 8, 'pin_memory': True}\n",
            "\u001b[2m\u001b[36m(pid=2361)\u001b[0m /usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "\u001b[2m\u001b[36m(pid=2361)\u001b[0m   cpuset_checked))\n",
            "\u001b[2m\u001b[36m(pid=2361)\u001b[0m 2021-10-01 11:58:40,530 ignite.distributed.auto.auto_dataloader INFO: Use data loader kwargs for dataset '<torch.utils.data.da': \n",
            "\u001b[2m\u001b[36m(pid=2361)\u001b[0m \t{'batch_size': 2, 'shuffle': True, 'num_workers': 8, 'pin_memory': True}\n",
            "\u001b[2m\u001b[36m(pid=2361)\u001b[0m /usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
            "\u001b[2m\u001b[36m(pid=2361)\u001b[0m   return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2m\u001b[36m(pid=2361)\u001b[0m Epoch[1], Iter[2000] Loss: 1.55 Running Avg Loss: 2.09\n",
            "\u001b[2m\u001b[36m(pid=2361)\u001b[0m Epoch[1], Iter[4000] Loss: 1.92 Running Avg Loss: 1.97\n",
            "\u001b[2m\u001b[36m(pid=2361)\u001b[0m Epoch[1], Iter[6000] Loss: 2.98 Running Avg Loss: 1.67\n",
            "\u001b[2m\u001b[36m(pid=2361)\u001b[0m Epoch[1], Iter[8000] Loss: 2.05 Running Avg Loss: 1.60\n",
            "\u001b[2m\u001b[36m(pid=2361)\u001b[0m Epoch[1], Iter[10000] Loss: 1.80 Running Avg Loss: 1.75\n",
            "\u001b[2m\u001b[36m(pid=2361)\u001b[0m Epoch[1], Iter[12000] Loss: 2.18 Running Avg Loss: 1.62\n",
            "\u001b[2m\u001b[36m(pid=2361)\u001b[0m Epoch[1], Iter[14000] Loss: 1.39 Running Avg Loss: 1.54\n",
            "\u001b[2m\u001b[36m(pid=2361)\u001b[0m Epoch[1], Iter[16000] Loss: 1.25 Running Avg Loss: 1.55\n",
            "\u001b[2m\u001b[36m(pid=2361)\u001b[0m Epoch[1], Iter[18000] Loss: 1.56 Running Avg Loss: 1.63\n",
            "\u001b[2m\u001b[36m(pid=2361)\u001b[0m Epoch[1], Iter[20000] Loss: 2.59 Running Avg Loss: 1.51\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(pid=2361)\u001b[0m [W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "\u001b[2m\u001b[36m(pid=2361)\u001b[0m [W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "\u001b[2m\u001b[36m(pid=2361)\u001b[0m [W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "\u001b[2m\u001b[36m(pid=2361)\u001b[0m [W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "\u001b[2m\u001b[36m(pid=2361)\u001b[0m [W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "\u001b[2m\u001b[36m(pid=2361)\u001b[0m [W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "\u001b[2m\u001b[36m(pid=2361)\u001b[0m [W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "\u001b[2m\u001b[36m(pid=2361)\u001b[0m [W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for DEFAULT_b50ea_00002:\n",
            "  accuracy: 0.4689\n",
            "  date: 2021-10-01_12-00-48\n",
            "  done: false\n",
            "  experiment_id: 1c5539c7b0e24aef998eaca10497f948\n",
            "  hostname: ac982acc27c2\n",
            "  iterations_since_restore: 1\n",
            "  loss: 1.46265712890625\n",
            "  node_ip: 172.28.0.2\n",
            "  pid: 2361\n",
            "  should_checkpoint: true\n",
            "  time_since_restore: 132.09271001815796\n",
            "  time_this_iter_s: 132.09271001815796\n",
            "  time_total_s: 132.09271001815796\n",
            "  timestamp: 1633089648\n",
            "  timesteps_since_restore: 0\n",
            "  training_iteration: 1\n",
            "  trial_id: b50ea_00002\n",
            "  \n",
            "== Status ==\n",
            "Memory usage on this node: 3.4/12.7 GiB\n",
            "Using AsyncHyperBand: num_stopped=2\n",
            "Bracket: Iter 2.000: -1.7122290039062502 | Iter 1.000: -1.46265712890625\n",
            "Resources requested: 2.0/2 CPUs, 1.0/1 GPUs, 0.0/7.33 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:K80)\n",
            "Result logdir: /root/ray_results/DEFAULT_2021-10-01_11-50-00\n",
            "Number of trials: 3/3 (1 RUNNING, 2 TERMINATED)\n",
            "+---------------------+------------+-----------------+--------------+------+------+------------+---------+------------+----------------------+\n",
            "| Trial name          | status     | loc             |   batch_size |   l1 |   l2 |         lr |    loss |   accuracy |   training_iteration |\n",
            "|---------------------+------------+-----------------+--------------+------+------+------------+---------+------------+----------------------|\n",
            "| DEFAULT_b50ea_00002 | RUNNING    | 172.28.0.2:2361 |            2 |   64 |   32 | 0.00122324 | 1.46266 |     0.4689 |                    1 |\n",
            "| DEFAULT_b50ea_00000 | TERMINATED |                 |            8 |   16 |    4 | 0.00059636 | 1.72452 |     0.3351 |                    3 |\n",
            "| DEFAULT_b50ea_00001 | TERMINATED |                 |            2 |   32 |   64 | 0.00074678 | 1.31592 |     0.5269 |                    3 |\n",
            "+---------------------+------------+-----------------+--------------+------+------+------------+---------+------------+----------------------+\n",
            "\n",
            "\n",
            "\u001b[2m\u001b[36m(pid=2361)\u001b[0m Validation Results - Epoch[1] Avg accuracy: 0.47 Avg loss: 1.46\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(pid=2361)\u001b[0m [W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "\u001b[2m\u001b[36m(pid=2361)\u001b[0m [W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "\u001b[2m\u001b[36m(pid=2361)\u001b[0m [W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "\u001b[2m\u001b[36m(pid=2361)\u001b[0m [W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "\u001b[2m\u001b[36m(pid=2361)\u001b[0m [W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "\u001b[2m\u001b[36m(pid=2361)\u001b[0m [W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "\u001b[2m\u001b[36m(pid=2361)\u001b[0m [W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "\u001b[2m\u001b[36m(pid=2361)\u001b[0m [W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2m\u001b[36m(pid=2361)\u001b[0m Epoch[2], Iter[22000] Loss: 1.63 Running Avg Loss: 1.37\n",
            "\u001b[2m\u001b[36m(pid=2361)\u001b[0m Epoch[2], Iter[24000] Loss: 1.70 Running Avg Loss: 1.43\n",
            "\u001b[2m\u001b[36m(pid=2361)\u001b[0m Epoch[2], Iter[26000] Loss: 1.49 Running Avg Loss: 1.42\n",
            "\u001b[2m\u001b[36m(pid=2361)\u001b[0m Epoch[2], Iter[28000] Loss: 1.42 Running Avg Loss: 1.58\n",
            "\u001b[2m\u001b[36m(pid=2361)\u001b[0m Epoch[2], Iter[30000] Loss: 2.66 Running Avg Loss: 1.47\n",
            "\u001b[2m\u001b[36m(pid=2361)\u001b[0m Epoch[2], Iter[32000] Loss: 1.87 Running Avg Loss: 1.50\n",
            "\u001b[2m\u001b[36m(pid=2361)\u001b[0m Epoch[2], Iter[34000] Loss: 1.32 Running Avg Loss: 1.41\n",
            "\u001b[2m\u001b[36m(pid=2361)\u001b[0m Epoch[2], Iter[36000] Loss: 0.78 Running Avg Loss: 1.49\n",
            "\u001b[2m\u001b[36m(pid=2361)\u001b[0m Epoch[2], Iter[38000] Loss: 1.86 Running Avg Loss: 1.37\n",
            "\u001b[2m\u001b[36m(pid=2361)\u001b[0m Epoch[2], Iter[40000] Loss: 3.33 Running Avg Loss: 1.53\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(pid=2361)\u001b[0m [W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "\u001b[2m\u001b[36m(pid=2361)\u001b[0m [W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "\u001b[2m\u001b[36m(pid=2361)\u001b[0m [W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "\u001b[2m\u001b[36m(pid=2361)\u001b[0m [W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "\u001b[2m\u001b[36m(pid=2361)\u001b[0m [W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "\u001b[2m\u001b[36m(pid=2361)\u001b[0m [W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "\u001b[2m\u001b[36m(pid=2361)\u001b[0m [W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "\u001b[2m\u001b[36m(pid=2361)\u001b[0m [W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for DEFAULT_b50ea_00002:\n",
            "  accuracy: 0.4984\n",
            "  date: 2021-10-01_12-02-57\n",
            "  done: false\n",
            "  experiment_id: 1c5539c7b0e24aef998eaca10497f948\n",
            "  hostname: ac982acc27c2\n",
            "  iterations_since_restore: 2\n",
            "  loss: 1.4066556640625\n",
            "  node_ip: 172.28.0.2\n",
            "  pid: 2361\n",
            "  should_checkpoint: true\n",
            "  time_since_restore: 260.71564173698425\n",
            "  time_this_iter_s: 128.6229317188263\n",
            "  time_total_s: 260.71564173698425\n",
            "  timestamp: 1633089777\n",
            "  timesteps_since_restore: 0\n",
            "  training_iteration: 2\n",
            "  trial_id: b50ea_00002\n",
            "  \n",
            "== Status ==\n",
            "Memory usage on this node: 3.4/12.7 GiB\n",
            "Using AsyncHyperBand: num_stopped=2\n",
            "Bracket: Iter 2.000: -1.4066556640625 | Iter 1.000: -1.46265712890625\n",
            "Resources requested: 2.0/2 CPUs, 1.0/1 GPUs, 0.0/7.33 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:K80)\n",
            "Result logdir: /root/ray_results/DEFAULT_2021-10-01_11-50-00\n",
            "Number of trials: 3/3 (1 RUNNING, 2 TERMINATED)\n",
            "+---------------------+------------+-----------------+--------------+------+------+------------+---------+------------+----------------------+\n",
            "| Trial name          | status     | loc             |   batch_size |   l1 |   l2 |         lr |    loss |   accuracy |   training_iteration |\n",
            "|---------------------+------------+-----------------+--------------+------+------+------------+---------+------------+----------------------|\n",
            "| DEFAULT_b50ea_00002 | RUNNING    | 172.28.0.2:2361 |            2 |   64 |   32 | 0.00122324 | 1.40666 |     0.4984 |                    2 |\n",
            "| DEFAULT_b50ea_00000 | TERMINATED |                 |            8 |   16 |    4 | 0.00059636 | 1.72452 |     0.3351 |                    3 |\n",
            "| DEFAULT_b50ea_00001 | TERMINATED |                 |            2 |   32 |   64 | 0.00074678 | 1.31592 |     0.5269 |                    3 |\n",
            "+---------------------+------------+-----------------+--------------+------+------+------------+---------+------------+----------------------+\n",
            "\n",
            "\n",
            "\u001b[2m\u001b[36m(pid=2361)\u001b[0m Validation Results - Epoch[2] Avg accuracy: 0.50 Avg loss: 1.41\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(pid=2361)\u001b[0m [W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "\u001b[2m\u001b[36m(pid=2361)\u001b[0m [W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "\u001b[2m\u001b[36m(pid=2361)\u001b[0m [W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "\u001b[2m\u001b[36m(pid=2361)\u001b[0m [W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "\u001b[2m\u001b[36m(pid=2361)\u001b[0m [W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "\u001b[2m\u001b[36m(pid=2361)\u001b[0m [W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "\u001b[2m\u001b[36m(pid=2361)\u001b[0m [W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "\u001b[2m\u001b[36m(pid=2361)\u001b[0m [W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2m\u001b[36m(pid=2361)\u001b[0m Epoch[3], Iter[42000] Loss: 2.52 Running Avg Loss: 1.34\n",
            "\u001b[2m\u001b[36m(pid=2361)\u001b[0m Epoch[3], Iter[44000] Loss: 1.35 Running Avg Loss: 1.32\n",
            "\u001b[2m\u001b[36m(pid=2361)\u001b[0m Epoch[3], Iter[46000] Loss: 1.74 Running Avg Loss: 1.36\n",
            "\u001b[2m\u001b[36m(pid=2361)\u001b[0m Epoch[3], Iter[48000] Loss: 1.13 Running Avg Loss: 1.45\n",
            "\u001b[2m\u001b[36m(pid=2361)\u001b[0m Epoch[3], Iter[50000] Loss: 1.30 Running Avg Loss: 1.40\n",
            "\u001b[2m\u001b[36m(pid=2361)\u001b[0m Epoch[3], Iter[52000] Loss: 1.87 Running Avg Loss: 1.24\n",
            "\u001b[2m\u001b[36m(pid=2361)\u001b[0m Epoch[3], Iter[54000] Loss: 0.37 Running Avg Loss: 1.19\n",
            "\u001b[2m\u001b[36m(pid=2361)\u001b[0m Epoch[3], Iter[56000] Loss: 0.92 Running Avg Loss: 1.37\n",
            "\u001b[2m\u001b[36m(pid=2361)\u001b[0m Epoch[3], Iter[58000] Loss: 0.80 Running Avg Loss: 1.36\n",
            "\u001b[2m\u001b[36m(pid=2361)\u001b[0m Epoch[3], Iter[60000] Loss: 1.18 Running Avg Loss: 1.38\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(pid=2361)\u001b[0m [W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "\u001b[2m\u001b[36m(pid=2361)\u001b[0m [W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "\u001b[2m\u001b[36m(pid=2361)\u001b[0m [W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "\u001b[2m\u001b[36m(pid=2361)\u001b[0m [W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "\u001b[2m\u001b[36m(pid=2361)\u001b[0m [W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "\u001b[2m\u001b[36m(pid=2361)\u001b[0m [W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "\u001b[2m\u001b[36m(pid=2361)\u001b[0m [W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "\u001b[2m\u001b[36m(pid=2361)\u001b[0m [W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
            "2021-10-01 12:05:05,896\tINFO tune.py:561 -- Total run time: 905.52 seconds (905.37 seconds for the tuning loop).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result for DEFAULT_b50ea_00002:\n",
            "  accuracy: 0.5291\n",
            "  date: 2021-10-01_12-05-05\n",
            "  done: true\n",
            "  experiment_id: 1c5539c7b0e24aef998eaca10497f948\n",
            "  hostname: ac982acc27c2\n",
            "  iterations_since_restore: 3\n",
            "  loss: 1.33420810546875\n",
            "  node_ip: 172.28.0.2\n",
            "  pid: 2361\n",
            "  should_checkpoint: true\n",
            "  time_since_restore: 389.19638776779175\n",
            "  time_this_iter_s: 128.4807460308075\n",
            "  time_total_s: 389.19638776779175\n",
            "  timestamp: 1633089905\n",
            "  timesteps_since_restore: 0\n",
            "  training_iteration: 3\n",
            "  trial_id: b50ea_00002\n",
            "  \n",
            "== Status ==\n",
            "Memory usage on this node: 3.4/12.7 GiB\n",
            "Using AsyncHyperBand: num_stopped=3\n",
            "Bracket: Iter 2.000: -1.4066556640625 | Iter 1.000: -1.46265712890625\n",
            "Resources requested: 2.0/2 CPUs, 1.0/1 GPUs, 0.0/7.33 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:K80)\n",
            "Result logdir: /root/ray_results/DEFAULT_2021-10-01_11-50-00\n",
            "Number of trials: 3/3 (1 RUNNING, 2 TERMINATED)\n",
            "+---------------------+------------+-----------------+--------------+------+------+------------+---------+------------+----------------------+\n",
            "| Trial name          | status     | loc             |   batch_size |   l1 |   l2 |         lr |    loss |   accuracy |   training_iteration |\n",
            "|---------------------+------------+-----------------+--------------+------+------+------------+---------+------------+----------------------|\n",
            "| DEFAULT_b50ea_00002 | RUNNING    | 172.28.0.2:2361 |            2 |   64 |   32 | 0.00122324 | 1.33421 |     0.5291 |                    3 |\n",
            "| DEFAULT_b50ea_00000 | TERMINATED |                 |            8 |   16 |    4 | 0.00059636 | 1.72452 |     0.3351 |                    3 |\n",
            "| DEFAULT_b50ea_00001 | TERMINATED |                 |            2 |   32 |   64 | 0.00074678 | 1.31592 |     0.5269 |                    3 |\n",
            "+---------------------+------------+-----------------+--------------+------+------+------------+---------+------------+----------------------+\n",
            "\n",
            "\n",
            "== Status ==\n",
            "Memory usage on this node: 3.4/12.7 GiB\n",
            "Using AsyncHyperBand: num_stopped=3\n",
            "Bracket: Iter 2.000: -1.4066556640625 | Iter 1.000: -1.46265712890625\n",
            "Resources requested: 0/2 CPUs, 0/1 GPUs, 0.0/7.33 GiB heap, 0.0/3.67 GiB objects (0.0/1.0 accelerator_type:K80)\n",
            "Result logdir: /root/ray_results/DEFAULT_2021-10-01_11-50-00\n",
            "Number of trials: 3/3 (3 TERMINATED)\n",
            "+---------------------+------------+-------+--------------+------+------+------------+---------+------------+----------------------+\n",
            "| Trial name          | status     | loc   |   batch_size |   l1 |   l2 |         lr |    loss |   accuracy |   training_iteration |\n",
            "|---------------------+------------+-------+--------------+------+------+------------+---------+------------+----------------------|\n",
            "| DEFAULT_b50ea_00000 | TERMINATED |       |            8 |   16 |    4 | 0.00059636 | 1.72452 |     0.3351 |                    3 |\n",
            "| DEFAULT_b50ea_00001 | TERMINATED |       |            2 |   32 |   64 | 0.00074678 | 1.31592 |     0.5269 |                    3 |\n",
            "| DEFAULT_b50ea_00002 | TERMINATED |       |            2 |   64 |   32 | 0.00122324 | 1.33421 |     0.5291 |                    3 |\n",
            "+---------------------+------------+-------+--------------+------+------+------------+---------+------------+----------------------+\n",
            "\n",
            "\n",
            "\u001b[2m\u001b[36m(pid=2361)\u001b[0m Validation Results - Epoch[3] Avg accuracy: 0.53 Avg loss: 1.33\n",
            "Best trial config: {'l1': 32, 'l2': 64, 'lr': 0.0007467798213220272, 'batch_size': 2}\n",
            "Best trial final validation loss: 1.315916015625\n",
            "Best trial final validation accuracy: 0.5269\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2021-10-01 12:05:07,794 ignite.distributed.auto.auto_dataloader INFO: Use data loader kwargs for dataset 'Dataset CIFAR10': \n",
            "\t{'batch_size': 4, 'shuffle': False, 'num_workers': 2, 'pin_memory': True}\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
            "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'Accuracy': 0.5379}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0j_ErUWhHpTl"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}